import * as pulumi from "@pulumi/pulumi";
import * as aws from "@pulumi/aws";
import * as k8s from "@pulumi/kubernetes";
import * as k8sInputs from "@pulumi/kubernetes/types/input";

import { Dashboard } from "./dashboard";
import { ServiceRole } from "./serviceRole";

/**
 * EBSVolumeType lists the set of volume types accepted by an EKS storage class.
 */
export type EBSVolumeType = "io1" | "gp2" | "sc1" | "st1";

/**
 * EKSStorageClass describes the inputs to a single Kubernetes StorageClass provisioned by AWS. Any number of storage
 * classes can be added to a cluster at creation time. One of these storage classes may be configured the default
 * storage class for the cluster.
 */
export interface EKSStorageClass {
    /**
     * The EBS volume type.
     */
    type: pulumi.Input<EBSVolumeType>;

    /**
     * The AWS zone or zones for the EBS volume. If zones is not specified, volumes are generally round-robin-ed across
     * all active zones where Kubernetes cluster has a node. zone and zones parameters must not be used at the same
     * time.
     */
    zones?: pulumi.Input<pulumi.Input<string>[]>;

    /**
     * I/O operations per second per GiB for "io1" volumes. The AWS volume plugin multiplies this with the size of a
     * requested volume to compute IOPS of the volume and caps the result at 20,000 IOPS.
     */
    iopsPerGb?: pulumi.Input<number>;

    /**
     * Denotes whether the EBS volume should be encrypted.
     */
    encrypted?: pulumi.Input<boolean>;

    /**
     * The full Amazon Resource Name of the key to use when encrypting the volume. If none is supplied but encrypted is
     * true, a key is generated by AWS.
     */
    kmsKeyId?: pulumi.Input<string>;

    /**
     * True if this storage class should be the default storage class for the cluster.
     */
    default?: pulumi.Input<boolean>;

    /**
    * AllowVolumeExpansion shows whether the storage class allow volume expand
    */
    allowVolumeExpansion?: pulumi.Input<boolean>

    /**
    * Standard object's metadata. More info:
    * https://git.k8s.io/community/contributors/devel/api-conventions.md#metadata
    */
    metadata?: pulumi.Input<k8sInputs.meta.v1.ObjectMeta>

    /**
    * Dynamically provisioned PersistentVolumes of this storage class are created with these
    * mountOptions, e.g. ["ro", "soft"]. Not validated - mount of the PVs will simply fail if one
    * is invalid.
    */
    mountOptions?: pulumi.Input<string[]>

    /**
    * Dynamically provisioned PersistentVolumes of this storage class are created with this
    * reclaimPolicy. Defaults to Delete.
    */
    reclaimPolicy?: pulumi.Input<string>

    /**
    * VolumeBindingMode indicates how PersistentVolumeClaims should be provisioned and bound.
    * When unset, VolumeBindingImmediate is used. This field is alpha-level and is only honored
    * by servers that enable the VolumeScheduling feature.
    */
    volumeBindingMode?: pulumi.Input<string>
}

/**
 * EKSClusterOptions describes the configuration options accepted by an EKSCluster component.
 */
export interface EKSClusterOptions {
    /**
     * The VPC in which to create the cluster and its worker nodes.
     */
    vpcId: pulumi.Input<string>;

    /**
     * The subnets to attach to the EKS cluster.
     */
    subnetIds: pulumi.Input<pulumi.Input<string>[]>;

    /**
     * The instance type to use for the cluster's nodes.
     */
    instanceType: pulumi.Input<aws.ec2.InstanceType>;

    /**
     * The number of worker nodes that should be running in the cluster.
     */
    desiredCapacity: pulumi.Input<number>;

    /**
     * The minimum number of worker nodes running in the cluster.
     */
    minSize: pulumi.Input<number>;

    /**
     * The maximum number of worker nodes running in the cluster.
     */
    maxSize: pulumi.Input<number>;

    /**
     * An optional set of StorageClasses to enable for the cluster. If this is a single volume type rather than a map,
     * a single StorageClass will be created for that volume type and made the cluster's default StorageClass.
     */
    storageClasses?: { [name: string]: EKSStorageClass } | EBSVolumeType;

    /**
     * Whether or not to deploy the Kubernetes dashboard to the cluster. If the dashboard is deployed, it can be
     * accessed as follows:
     * 1. Retrieve an authentication token for the dashboard by running the following and copying the value of `token`
     *   from the output of the last command:
     *
     *     $ kubectl -n kube-system get secret | grep eks-admin | awk '{print $1}'
     *     $ kubectl -n kube-system describe secret <output from previous command>
     *
     * 2. Start the kubectl proxt:
     *
     *     $ kubectl proxy
     *
     * 3. Open `http://localhost:8001/api/v1/namespaces/kube-system/services/https:kubernetes-dashboard:/proxy/` in a
     *    web browser.
     * 4. Choose `Token` authentication, paste the token retrieved earlier into the `Token` field, and sign in.
     */
    deployDashboard?: boolean;
}

// createStorageClass creates a single StorageClass from the given inputs.
function createStorageClass(name: string, storageClass: EKSStorageClass, opts: pulumi.CustomResourceOptions) {
    // Compute the storage class's metadata, including its name and default storage class annotation.
    const metadata = pulumi.all([storageClass.metadata || {}, storageClass.default])
        .apply(([m, isDefault]) => {
            m.name = m.name || name;
            if (isDefault) {
                m.annotations = { ...m.annotations || {}, "storageclass.kubernetes.io/is-default-class":"true" };
            }
            return m;
        });

    // Figure out the parameters for the storage class.
    const parameters: { [key: string]: pulumi.Input<string> } = {
        "type": storageClass.type,
    };
    if (storageClass.zones) {
        parameters["zones"] = pulumi.output(storageClass.zones).apply(v => v.join(", "));
    }
    if (storageClass.iopsPerGb) {
        parameters["iopsPerGb"] = pulumi.output(storageClass.iopsPerGb).apply(v => `${v}`);
    }
    if (storageClass.encrypted) {
        parameters["encrypted"] = pulumi.output(storageClass.encrypted).apply(v => `${v}`);
    }
    if (storageClass.kmsKeyId) {
        parameters["kmsKeyId"] = storageClass.kmsKeyId;
    }

    new k8s.storage.v1.StorageClass(name, {
        metadata: metadata,
        provisioner: "kubernetes.io/aws-ebs",
        parameters: parameters,
        allowVolumeExpansion: storageClass.allowVolumeExpansion,
        mountOptions: storageClass.mountOptions,
        reclaimPolicy: storageClass.reclaimPolicy,
        volumeBindingMode: storageClass.volumeBindingMode,
    }, opts);
}

/**
 * EKSCluster is a component that wraps the AWS and Kubernetes resources necessary to run an EKS cluster, its worker
 * nodes, its optional StorageClasses, and an optional deployment of the Kubernetes Dashboard.
 */
export class EKSCluster extends pulumi.ComponentResource {
    /**
     * A kubeconfig that can be used to connect to the EKS cluster. This must be serialized as a string before passing
     * to the Kubernetes provider.
     */
    public readonly kubeconfig: pulumi.Output<any>;

    /**
     * Create a new EKS cluster with worker nodes, optional storage classes, and deploy the Kubernetes Dashboard if
     * requested.
     *
     * @param name The _unique_ name of this component.
     * @param args The arguments for this cluster.
     * @param opts A bag of options that control this copmonent's behavior.
     */
    constructor(name: string, args: EKSClusterOptions, opts?: pulumi.ComponentResourceOptions) {
        super("EKSCluster", name, args, opts);

        // Create the EKS service role
        const eksRole = new ServiceRole("eksRole", {
            service: "eks.amazonaws.com",
            description: "Allows EKS to manage clusters on your behalf.",
            managedPolicyArns: [
                "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy",
                "arn:aws:iam::aws:policy/AmazonEKSServicePolicy",
            ],
        }, { parent: this });

        // Create the EKS cluster security group
        const allEgress = {
            description: "Allow internet access.",
            fromPort: 0,
            toPort: 0,
            protocol: "-1",  // all
            cidrBlocks: [ "0.0.0.0/0" ],
        };
        const eksClusterSecurityGroup = new aws.ec2.SecurityGroup("eksClusterSecurityGroup", {
            vpcId: args.vpcId,
            egress: [ allEgress ],
        }, { parent: this });

        // Create the EKS cluster
        const eksCluster = new aws.eks.Cluster("eksCluster", {
            roleArn: eksRole.role.apply(r => r.arn),
            vpcConfig: { securityGroupIds: [ eksClusterSecurityGroup.id ], subnetIds: args.subnetIds },
        }, { parent: this });

        // Create the instance role we'll use for worker nodes.
        const instanceRole = new ServiceRole("instanceRole", {
            service: "ec2.amazonaws.com",
            managedPolicyArns: [
                "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy",
                "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy",
                "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly",
            ],
        }, { parent: this });
        const instanceRoleARN = instanceRole.role.apply(r => r.arn);

        // Compute the required kubeconfig. Note that we do not export this value: we want the exported config to
        // depend on the autoscaling group we'll create later so that nothing attempts to use the EKS cluster before
        // its worker nodes have come up.
        const myKubeconfig = pulumi.all([eksCluster.name, eksCluster.endpoint, eksCluster.certificateAuthority])
            .apply(([clusterName, clusterEndpoint, clusterCertificateAuthority]) => {
                return {
                    apiVersion: "v1",
                    clusters: [{
                        cluster: {
                            server: clusterEndpoint,
                            "certificate-authority-data": clusterCertificateAuthority.data,
                        },
                        name: "kubernetes",
                    }],
                    contexts: [{
                        context: {
                            cluster: "kubernetes",
                            user: "aws",
                        },
                        name: "aws",
                    }],
                    "current-context": "aws",
                    kind: "Config",
                    users: [{
                        name: "aws",
                        user: {
                            exec: {
                                apiVersion: "client.authentication.k8s.io/v1alpha1",
                                command: "aws-iam-authenticator",
                                args: ["token", "-i", clusterName],
                            },
                        },
                    }],
                };
            });

        // Create the Kubernetes provider we'll use to manage the config map we need to allow worker nodes to access
        // the EKS cluster.
        const k8sProvider = new k8s.Provider("eks-k8s", {
            kubeconfig: myKubeconfig.apply(JSON.stringify),
        }, { parent: this });

        // Enable access to the EKS cluster for worker nodes.
        const eksNodeAccess = new k8s.core.v1.ConfigMap("nodeAccess", {
            apiVersion: "v1",
            metadata: {
                name: "aws-auth",
                namespace: "kube-system",
            },
            data: {
                mapRoles: instanceRoleARN.apply(arn => `- rolearn: ${arn}\n  username: system:node:{{EC2PrivateDNSName}}\n  groups:\n    - system:bootstrappers\n    - system:nodes\n`),
            },
        }, { parent: this, provider: k8sProvider });

        // Add any requested StorageClasses.
        if (args.storageClasses) {
            if (typeof args.storageClasses === "string") {
                const storageClass = { type: args.storageClasses, default: true };
                createStorageClass(args.storageClasses, storageClass, { parent: this, provider: k8sProvider });
            } else {
                for (const name of Object.keys(args.storageClasses)) {
                    createStorageClass(name, args.storageClasses[name], { parent: this, provider: k8sProvider });
                }
            }
        }

        // Create the cluster's worker nodes.
        const instanceProfile = new aws.iam.InstanceProfile("instanceProfile", {
            role: instanceRole.role,
        }, { parent: this });
        const instanceSecurityGroup = new aws.ec2.SecurityGroup("instanceSecurityGroup", {
            vpcId: args.vpcId,
            ingress: [
                {
                    description: "Allow nodes to communicate with each other",
                    fromPort: 0,
                    toPort: 0,
                    protocol: "-1", // all
                    self: true,
                },
                {
                    description: "Allow worker Kubelets and pods to receive communication from the cluster control plane",
                    fromPort: 1025,
                    toPort: 65535,
                    protocol: "tcp",
                    securityGroups: [ eksClusterSecurityGroup.id ],
                },
            ],
            egress: [ allEgress ],
            tags: eksCluster.name.apply(n => <aws.Tags>{
                [`kubernetes.io/cluster/${n}`]: "owned",
            }),
        }, { parent: this });
        const eksClusterIngressRule = new aws.ec2.SecurityGroupRule("eksClusterIngressRule", {
            description: "Allow pods to communicate with the cluster API Server",
            type: "ingress",
            fromPort: 443,
            toPort: 443,
            protocol: "tcp",
            securityGroupId: eksClusterSecurityGroup.id,
            sourceSecurityGroupId: instanceSecurityGroup.id,
        }, { parent: this });
        const instanceSecurityGroupId = pulumi.all([instanceSecurityGroup.id, eksClusterIngressRule.id])
            .apply(([instanceSecurityGroupId]) => instanceSecurityGroupId);

        const awsRegion = pulumi.output(aws.getRegion({}, { parent: this }));
        const userdata = pulumi.all([awsRegion, eksCluster.name, eksCluster.endpoint, eksCluster.certificateAuthority])
            .apply(([region, clusterName, clusterEndpoint, clusterCertificateAuthority]) => {
                return `#!/bin/bash -xe

CA_CERTIFICATE_DIRECTORY=/etc/kubernetes/pki
CA_CERTIFICATE_FILE_PATH=$CA_CERTIFICATE_DIRECTORY/ca.crt
mkdir -p $CA_CERTIFICATE_DIRECTORY
echo "${clusterCertificateAuthority.data}" | base64 -d >  $CA_CERTIFICATE_FILE_PATH
INTERNAL_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)
sed -i s,MASTER_ENDPOINT,${clusterEndpoint},g /var/lib/kubelet/kubeconfig
sed -i s,CLUSTER_NAME,${clusterName},g /var/lib/kubelet/kubeconfig
sed -i s,REGION,${region.name},g /etc/systemd/system/kubelet.service
sed -i s,MAX_PODS,20,g /etc/systemd/system/kubelet.service
sed -i s,MASTER_ENDPOINT,${clusterEndpoint},g /etc/systemd/system/kubelet.service
sed -i s,INTERNAL_IP,$INTERNAL_IP,g /etc/systemd/system/kubelet.service
DNS_CLUSTER_IP=10.100.0.10
if [[ $INTERNAL_IP == 10.* ]] ; then DNS_CLUSTER_IP=172.20.0.10; fi
sed -i s,DNS_CLUSTER_IP,$DNS_CLUSTER_IP,g /etc/systemd/system/kubelet.service
sed -i s,CERTIFICATE_AUTHORITY_FILE,$CA_CERTIFICATE_FILE_PATH,g /var/lib/kubelet/kubeconfig
sed -i s,CLIENT_CA_FILE,$CA_CERTIFICATE_FILE_PATH,g  /etc/systemd/system/kubelet.service
systemctl daemon-reload
systemctl restart kubelet kube-proxy
`;
            });
        const eksWorkerAmi = aws.getAmi({
            filters: [{
                name: "name",
                values: [ "eks-worker-*" ],
            }],
            mostRecent: true,
            owners: [ "602401143452" ], // Amazon
        }, { parent: this });
        const instanceLaunchConfiguration = new aws.ec2.LaunchConfiguration("instanceLaunchConfiguration", {
            associatePublicIpAddress: true,
            imageId: eksWorkerAmi.then(r => r.imageId),
            instanceType: args.instanceType,
            iamInstanceProfile: instanceProfile.id,
            securityGroups: [ instanceSecurityGroupId ],
            userData: userdata,
        }, { parent: this });
        const autoscalingGroup = new aws.autoscaling.Group("autoscalingGroup", {
            desiredCapacity: args.desiredCapacity,
            launchConfiguration: instanceLaunchConfiguration.id,
            maxSize: args.maxSize,
            minSize: args.minSize,
            vpcZoneIdentifiers: args.subnetIds,

            tags: [
                {
                    key: eksCluster.name.apply(n => `kubernetes.io/cluster/${n}`),
                    value: "owned",
                    propagateAtLaunch: true,
                },
                {
                    key: "Name",
                    value: eksCluster.name.apply(n => `${n}-worker`),
                    propagateAtLaunch: true,
                }
            ]

        }, { parent: this, dependsOn: eksNodeAccess });

        // Export the cluster's kubeconfig with a dependency upon the cluster's autoscaling group. This will help
        // ensure that the cluster's consumers do not attempt to use the cluster until its workers are attached.
        this.kubeconfig = pulumi.all([autoscalingGroup.id, myKubeconfig]).apply(([_, kubeconfig]) => kubeconfig);

        // If we need to deploy the Kubernetes dashboard, do so now.
        if (args.deployDashboard) {
            // Deploy the dashboard and its dependencies.
            const dashboard = new Dashboard("dashboard", {
                parent: this,
                dependsOn: autoscalingGroup,
                providers: { kubernetes: k8sProvider },
            });

            // Create a service account for admin access.
            const adminAccount = new k8s.core.v1.ServiceAccount("eks-admin", {
                metadata: {
                    name: "eks-admin",
                    namespace: "kube-system",
                },
            }, { parent: this, dependsOn: autoscalingGroup, provider: k8sProvider });

            // Create a role binding for the admin account.
            const adminRoleBinding = new k8s.rbac.v1.ClusterRoleBinding("eks-admin", {
                metadata: {
                    name: "eks-admin",
                },
                roleRef: {
                    apiGroup: "rbac.authorization.k8s.io",
                    kind: "ClusterRole",
                    name: "cluster-admin",
                },
                subjects: [{
                    kind: "ServiceAccount",
                    name: "eks-admin",
                    namespace: "kube-system",
                }],
            }, { parent: this, dependsOn: autoscalingGroup, provider: k8sProvider });
        }

        this.registerOutputs({ kubeconfig: this.kubeconfig });
    }
}
